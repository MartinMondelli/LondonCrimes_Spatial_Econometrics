---
title: "Spatial Econometrics"
subtitle: "Spatial Point Patterns"
author: "Mondelli Martin, Senseby Dorian,
 Francisco Gonzalez-Garcia, Barcaroli Clement"
date: "`r Sys.Date()`"

format:
  html:
    html-math-method: katex
    theme: journal      
    toc: true
    toc-title: "Table of Contents"
    toc-depth: 3
    number-sections: false
    code-fold: show
    code-summary: "hide code"
    



execute:
  warning: false
  message: false

editor:
  visual:
    wrap: 72
---

```{r setup, include=FALSE}

if (!require(knitr))      install.packages("knitr")

library(knitr)

knitr::opts_chunk$set(
  echo = TRUE,       # afficher le code par défaut
  results = "hold",  # tous les outputs ensemble
  fig.show = "hold", # tous les graphiques ensemble
  warning = FALSE,
  message = FALSE
)

```

Importing data and packages

```{r}
# install les packages necessaires si ils ne sont pas deja installes
if (!require(sf))      install.packages("sf")
if (!require(mapview))      install.packages("mapview")
if (!require(rnaturalearth))      install.packages("rnaturalearth")
if (!require(spatstat))      install.packages("spatstat")
if (!require(leaflet))      install.packages("leaflet")


# charge les packages
library("sf")
library("mapview")
library(rnaturalearth)
library(spatstat)
library(leaflet)


# chemin adaptatif vers les données 
path_data = paste(getwd(), "2025-07-metropolitan-outcomes.csv", sep = "/")

police = read.csv(path_data)

```

Counting number of NAs

```{r}
sum(is.na(police$Longitude))
sum(is.na(police$Latitude))
```

Filtering and we keep only police_clean

```{r}
police_clean <- police[!is.na(police$Longitude) & !is.na(police$Latitude),]
rm(police, path_data)
gc()
```

Making it sf

```{r}
my_coords <- st_as_sf(police_clean, coords = c("Longitude", "Latitude"), crs = 4326)
my_coords <- st_transform(my_coords, 27700)
```

Plot it

```{r}
mapview(my_coords) #If it doesn't work try with leaflet
```

```{r}
coords <- st_coordinates(my_coords)
win <- as.owin(st_bbox(my_coords))
police_clean_ppp <- ppp(
  x = coords[,1],
  y = coords[,2],
  window = win
)
```

# Data base presentation

# I/ Spatial Point Patterns Introduction

# II/ Complete Spatial Randomness

# III/ Intensity estimation

The intensity function of a spatial point process $\{X_1, X_2, \dots, X_{N(A)}\}$ in a planar window $A \subset \mathbb{R}^2$ is defined as:

$$
\lambda(x) = \lim_{|dx| \to 0} \frac{\mathbb{E}[N(dx)]}{|dx|},
$$

where $dx$ is a small region containing the point $x$. Under stationary, the distribution of the process is invariant under spatial translations, and under isotropic it is invariant under rotations; therefore, the expected number of events per unit area does not depend on the location $x$, implying that the intensity is constant and equal to $\lambda(x) = \lambda = \frac{\mathbb{E}[N(A)]}{|A|}$.

Thus, for an observed spatial point pattern of $n$ events observed in a region $A$, the intensity can be estimated as:

$$
\hat{\lambda} = \frac{n}{|A|}.
$$

The density and intensity functions are proportional:

$$
\lambda(x) = f(x) \int_A \lambda(u)\, du,
$$

where $\int_A \lambda(u)\, du$ is the expected number of events in $A$. So the intensity is equal to the probability to observe an event at location $x$ multiply by the expected number of event in $A$

$$
\hat{f}(x) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{h^2} 
K\!\left( \frac{x - x_i}{h} \right).
$$

$$
\hat{\lambda}(x) = \sum_{i=1}^{n} \frac{1}{h^2} 
K\!\left( \frac{x - x_i}{h} \right).
$$

where $K(\cdot)$ is a symmetric function such that $K(x) \ge 0 \quad \forall x$ and $\int_A K(x)\, dx = 1,$ and $h$ is a smoothing parameter known as the bandwidth.

where $K(\cdot)$ is a symmetric function such that $K(x) \ge 0 \quad \forall x$ and $\int_{\mathbb{R}^2} K(u)\, du = 1$.

If a Gaussian kernel is used in two dimensions, then

$$
K(u) = \frac{1}{2\pi} 
\exp\left(-\frac{1}{2} u^\top u \right),
\quad u \in \mathbb{R}^2,
$$

where $u^\top u = \|u\|^2 = u_1^2 + u_2^2$.

One edge-correction term is defined as:

$$
p_h(x) = \int_A h^{-2} 
K\!\left( \frac{x - u}{h} \right) du
$$


$p_h(x)$ represents the proportion of the kernel centered at $x$ that lies within the study region $A$ to correct the negative bias that occurs near the boundaries.


$$ 
\longrightarrow \hat{f}(x) = \frac{1}{n p_h(x)}  \sum_{i=1}^{n} \frac{1}{h^2} 
K\!\left( \frac{x - x_i}{h} \right)
$$

and

$$
\longrightarrow \hat{\lambda}(x) = \frac{1}{p_h(x)}  \sum_{i=1}^{n} \frac{1}{h^2} 
K\!\left( \frac{x - x_i}{h} \right)
$$

```{r}

my_coords2 <- st_as_sf(police_clean, coords = c("Longitude", "Latitude"), crs = 4326)


my_coords2 = st_transform(my_coords2, crs = 27700)
# Coordonnées
coords2 <- st_coordinates(my_coords2)



map <- ne_countries(type = "countries", country = "United Kingdom",
                    scale = "small", returnclass = "sf")


map <- st_transform(map, crs = "EPSG:27700")
win <- as.owin(map)


pp <- ppp(
  x = coords2[,1],
  y = coords2[,2],
  window = win
)


par(mfrow = c(1,1),
    mar = c(1,1,1,1))  # bas, gauche, haut, droite


# Plot
plot(win, col = "lightgrey", main = "Spatial Point Pattern of Police Outcomes in the UK")
plot(pp, add = TRUE, pch = 20, col = "blue",size = 0.8)




```

## Intensity estimation

Possible kernels are : gaussian, epanechnikov, quartic, disc or a personalized function

Estimation with the default bandwidth and gaussian kernel:

```{r}
# kernel gaussian
lambdahat <- density(pp)
attr(lambdahat, "sigma")
```

Interpretation:

Sigma = bandwidth, it is in meters so a point has an influence on intensity in a zone with 76 km of diameter

```{r}
par(mfrow = c(2,2),
    mar = c(1,1,1,1),      # marges internes des graphiques
    oma = c(0,0,2,0))      # marges externes (haut = 4)

plot(density(pp, sigma = 10000),
     main = "Bandwidth = 10000")

plot(density(pp, sigma = 20000),
     main = "Bandwidth = 20000")

plot(density(pp, sigma = 50000),
     main = "Bandwidth = 50000")

plot(density(pp),
     main = "Default Bandwidth = 76634.26")

# Titre global
mtext("Intensity estimation with different bandwidths",
      outer = TRUE,
      cex = 1.5,
      font = 2)


```

```{r}
table_var = list()
for (i in colnames(police_clean)) {
  table_var[[i]] = table(police_clean[i])
}
```

```{r}

table_var$Outcome.type

```

# IV/ The k-function

The K-function is defined as :

$$
K(s) = \frac{E(\text{number of n events within distance s of event i})}{\lambda}
$$
Where $\lambda$ is the intensity function. As mentioned before, it is defined, in the case of CSR as:

$$
\lambda = \frac{E(N(A))}{|A|}
$$
Where N(A) is the number of points in A and $|A|$ is the area of A. 

We can also rewrite the "number of n events within distance s of event i" as the number of points within a circle C of radius s and center i or more concise within region A: "number of points in C". Then, we can rewrite the K-function as:

$$
K(s) = \frac{E(\text{number of points in C})\times\frac{|C|}{|C|}}{\frac{E(\text{number of points in C)} }{|C|}}
$$
Which simplifies to : $K(s) = |C|=\pi\times s^2$.

As we mentioned before, CSR is our benchmark: this scenario means that all points are distributed randomly and there is no clustering or regular patterns. Meaning that points are neither too close or too separated. This interpretation is due to the fact that if we had a clustering situation while we suppose CSR, then it would be highly likely that there will be more points than expected under CSR close to a certain event i, thus driving the expectation in the numerator above the CSR scenario. In the case of regular pattern, we would have the opposite situation, there would be points that are quite separeted driving the number of expected points arround a certain event i down and thus decreasing K.   

Before going to the application, let's define the estimator function of K when we do not assume CSR:

$$
\hat{K}(s) = \frac{\hat{E}(s)}{\hat{\lambda}}
$$
Where:
$$
\hat{E}(s)=\frac{1}{n}\sum_{i=1}^n\sum_{i \neq j}^{n-1}w_{ij}I(d_{ij}\le s)
$$
and $\hat{\lambda}$ the estimator of the intensity function as defined before.

This estimator is a quite forward estimation, in fact for the numerator we just count the number of points we actually see in the circle C for all possible points and we then take the mean to retrieve the average number of points in the circle C when we take different centroids. This gives us a clear idea of the number of neighboring events for each point on average. For the denominator, we just take the usual estimator for the intensity function as we saw before. 

About the weights, we need them because there are certain events that cannot be considered as the others. For example, if we consider an event i that is at border of A, if we do not take into consideration the fact that a part of the circle C is located outside the region and thus has 0 probability of containing other events, we would have a negative bias for these points. Another reason to consider this weights is because: we might be considering a circle C that is too small or too large: in fact if we take a circle so small that cannot contain any other event, then we would have a negative bias. Inversely, if the circle is too big, we might be considering as neighbors events that are too far apart, thus producing a positive bias.

The weights we can apply in the package spatstat are:

- The border correction (Ripley 1988): $w_{ij}=1$ if C fits A, else it is equal to 0 (Ripley, 1988)

- The isotropic correction (Ohser 1983, Ripley 1988): $w_{ij} = \frac{\text{Total area of C}}{\text{Are of C in A}}$

-The translate correction (Ohser 1983): 
$$
w_{ij} = \frac{|A|}{|A \cap (A+(x_i-x_j))|}
$$
It exist also the possibility to compute $\hat{K}(s)$ without $w_{i,j}$.

```{r}
K <- Kest(
  police_clean_ppp,
  correction = c("border", "Ripley", "isotropic", "translate")
)
plot(K)
```

The function K is giving us the mean of all possible circles of size r for each r between 0 and 25 km. We clearly see that we are above the benchmark of the theoretical K (when we apply CSR) whatoever the weights, then we can conclude that points are clearly clustered.

##The L-function

A variant of the $\hat{K}(s)$ function is the $\hat{L}(s)$ that is the square root of the first one divided by square root of $\pi$. This variant allows to interpret better the measure of the K-function and gives us better visualizations.

$$
L(s) = \sqrt{\frac{K(s)}{\pi}}
$$
and 
$$
\hat{L}(s) = \sqrt{\frac{\hat{K}(s)}{\pi}}

$$

```{r}
L <- Lest(police_clean_ppp,
          correction = c("border", "Ripley", "isotropic", "translate")
)
plot(L)
```

We have the same interpretations for the L-function.

##The K-function and Monte Carlo

It is also possible to test for CSR using the K function and a Monte Carlo approximation:

- Compute the $\hat{K}(s)$ of the real data.
- Generate M number of spatial point patterns over A using homogeneous Poisson process (CSR).
- Estimate $\hat{K}(s)$ and take the confidence interval of size 95\%.
- Reject CSR if the real estimation is outside of the CI.

This procedures gives us a more significant measure to test for CSR. In fact, it allows us (or not) to reject at confidence level 95% the precense of CSR.

```{r}
E <- envelope(police_clean_ppp, Kest, nsim = 99)
plot(E)
```

##Conclusion
Clearly the pattern is not random as the poisson distribution is clearly lower than the empirical K. The estimators (with the different corrections) are quite high meaning that the point pattern is clearly not random. For small radius we observe that the clustering is already important but it clearly explodes after 5 km.

Remark: one limitation, as we could see on the map above is that the points are quite centered in London and surroundings meaning that our estimation is driven mainly by this area.


# Conclusion
